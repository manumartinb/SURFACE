# üìä DIAGN√ìSTICO COMPLETO: V19 PERMA SURFACE
## An√°lisis Exhaustivo de Arquitectura, Procesos Matem√°ticos y Errores

**Fecha de An√°lisis:** 2025-11-28
**Versi√≥n Analizada:** V19_rev2 [PERMA SURFACE]
**Analista:** Claude Code - An√°lisis Matem√°tico Exhaustivo
**Nivel de Profundidad:** M√ÅXIMO

---

## üéØ RESUMEN EJECUTIVO

### Objetivo del Sistema
El script V19 PERMA SURFACE tiene como objetivo **etiquetar c√≥mo de cara o barata est√° un contrato de opciones dentro de un bucket espec√≠fico respecto de sus ventanas hist√≥ricas** mediante:

1. **Clasificaci√≥n en buckets** (Delta √ó DTE)
2. **C√°lculo de percentiles hist√≥ricos** sobre calendario universal USA
3. **Scores combinados** (IV + SKEW + VRP) con pesos configurables
4. **Etiquetado en 10 niveles** desde "ULTRA_BARATA" hasta "ULTRA_CARA"

### Veredicto General
**üü° ESTADO: FUNCIONAL CON ERRORES MATEM√ÅTICOS CR√çTICOS**

- ‚úÖ **Fortalezas:** Arquitectura s√≥lida, manejo robusto de datos, validaciones extensivas
- üî¥ **Cr√≠tico:** 3 errores matem√°ticos graves que afectan la precisi√≥n de percentiles y scores
- üü° **Advertencias:** 8 inconsistencias menores y posibles mejoras de robustez

---

## üìê ARQUITECTURA DEL SISTEMA

### 1. Estructura de Procesamiento

```
INPUT: 30MINDATA_*.csv
  ‚Üì
[1. CARGA Y VALIDACI√ìN]
  - Validaci√≥n de esquema
  - Normalizaci√≥n ms_of_day
  - Filtros de calidad
  ‚Üì
[2. FILTRADO TEMPORAL]
  - Snapshot 10:00 AM (¬± 90s)
  - Snapshot 15:30 PM (¬± 60s)
  ‚Üì
[3. BUCKETING]
  - 10 buckets Delta (4Œî ‚Üí 60Œî)
  - 22 buckets DTE (2d ‚Üí 1390d)
  - Total: 440 buckets por wing
  ‚Üì
[4. AGREGACI√ìN POR BUCKET]
  - Interpolaci√≥n a punto fijo
  - Expansi√≥n a vecinos si insuficientes datos
  - C√°lculo de m√©tricas: IV, SKEW, TERM
  ‚Üì
[5. FORWARD-FILL CONTROLADO]
  - Reindex a calendario completo USA
  - Forward-fill m√°ximo 30 d√≠as
  - Marcado de datos reales vs rellenos
  ‚Üì
[6. C√ÅLCULO DE M√âTRICAS ROLLING]
  - HV (7D, 21D, 63D, 252D)
  - VRP = IV_ATM - HV_7D(t-1)
  - Z-scores de IV (20D, 63D, 252D)
  ‚Üì
[7. PERCENTILES HIST√ìRICOS]
  - Ventanas: 7D, 21D, 63D, 252D
  - Calendario universal USA
  - Solo sobre datos reales
  ‚Üì
[8. SCORES COMBINADOS]
  - SCORE = 0.60√óIV_pct + 0.35√óSKEW_pct + 0.05√óVRP_pct
  - Excepci√≥n ATM (40-60Œî): solo IV + VRP
  ‚Üì
[9. CLASIFICACI√ìN]
  - Level 1-10
  - Labels: ULTRA_BARATA ‚Üí ULTRA_CARA
  ‚Üì
OUTPUT: surface_metrics.parquet
```

### 2. Configuraci√≥n Clave

| Par√°metro | Valor | Prop√≥sito |
|-----------|-------|-----------|
| `SCORE_WEIGHTS` | (0.60, 0.35, 0.05) | Pesos para IV, SKEW, VRP |
| `WINDOWS` | [7, 21, 63, 252] | Ventanas para percentiles (d√≠as trading) |
| `N_MIN_PER_BUCKET` | 3 | M√≠nimo contratos para bucket v√°lido |
| `MAX_FFILL_DAYS` | 30 | M√°ximo forward-fill permitido |
| `MIN_PERCENTILE_COVERAGE` | 0.70 | Cobertura m√≠nima para percentil v√°lido |
| `NEIGHBOR_DELTA_EXPAND` | 5.0 | Expansi√≥n Delta si datos insuficientes |
| `NEIGHBOR_DTE_EXPAND` | 5 | Expansi√≥n DTE si datos insuficientes |

---

## üî¨ AN√ÅLISIS MATEM√ÅTICO EXHAUSTIVO

### PROCESO 1: Normalizaci√≥n de ms_of_day

**Ubicaci√≥n:** L√≠neas 260-286
**Complejidad:** ‚≠ê BAJA
**Funci√≥n:** `normalize_ms_of_day()`

#### Algoritmo
```python
1. Detectar formato: string "HH:MM:SS" vs num√©rico
2. Si string ‚Üí convertir a ms usando regex
3. Si num√©rico:
   - Si max ‚â§ 1445 ‚Üí minutos ‚Üí √ó60000
   - Si max ‚â§ 86410 ‚Üí segundos ‚Üí √ó1000
   - Si max > 200M ‚Üí sobrepasado ‚Üí √∑1000 iterativo
4. Clip a [0, 86_400_000]
5. Round y convertir a Int64
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **Robustez:** ALTA
- **Manejo de errores:** EXCELENTE (cubre m√∫ltiples formatos)

---

### PROCESO 2: C√°lculo de DTE (Days to Expiration)

**Ubicaci√≥n:** L√≠neas 300-304
**Complejidad:** ‚≠ê BAJA
**Funci√≥n:** `compute_dte_days()`

#### Algoritmo
```python
dte_days = (expiration_date - current_date).days
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **Nota:** Usa calendario real (no trading days), apropiado para DTE

---

### PROCESO 3: Bucketing (Delta √ó DTE)

**Ubicaci√≥n:** L√≠neas 107-146 (config), 1420-1429, 1452-1466 (asignaci√≥n)
**Complejidad:** ‚≠ê‚≠ê MEDIA

#### Definici√≥n de Buckets

**Delta Buckets (10):**
```python
[3.0, 5.5) ‚Üí d4   (rep=4)
[5.5, 8.5) ‚Üí d7   (rep=7)
...
[57.5, 65.0] ‚Üí d60 (rep=60)  # ‚ö†Ô∏è Inclusivo en high
```

**DTE Buckets (22):**
```python
[1, 3.5) ‚Üí t2    (rep=2)
...
[1280, 1500] ‚Üí t1390 (rep=1390)
```

#### L√≥gica de Asignaci√≥n
```python
# Para todos excepto √∫ltimo bucket
if tb["low"] <= dte < tb["high"]:
    asignar bucket

# Para √∫ltimo bucket (especial)
if tb["low"] <= dte <= tb["high"]:
    asignar bucket
```

#### üü° DIAGN√ìSTICO

**ADVERTENCIA MENOR: Inconsistencia en bordes**

**Problema:**
- Buckets intermedios usan `[low, high)` (semi-abierto derecha)
- √öltimo bucket usa `[low, high]` (cerrado)
- En l√≠nea 1467 hay c√≥digo duplicado para manejar esto

**Impacto:**
- BAJO: Solo afecta contratos exactamente en los l√≠mites
- Ejemplo: DTE exactamente = 1500 d√≠as

**Severidad:** üü° BAJA
**Recomendaci√≥n:** Unificar criterio usando siempre `<` y ajustar `high` del √∫ltimo bucket

---

### PROCESO 4: Expansi√≥n a Vecinos

**Ubicaci√≥n:** L√≠neas 454-500
**Complejidad:** ‚≠ê‚≠ê MEDIA
**Funci√≥n:** `expand_to_neighbors()`

#### Algoritmo
```python
1. Filtrar contratos en bucket [low, high)
2. Si len(sub) >= MIN_REQUIRED (8):
   ‚úì Retornar (expansion_level=0)
3. Si len(sub) < 8:
   ‚Üí Expandir rangos:
      Delta: [low-5, high+5]
      DTE: [low-5, high+5]
   ‚Üí Buscar contratos en rango expandido
4. Si len(expandido) >= 8:
   ‚úì Retornar expandido (expansion_level=1)
5. Si no:
   ‚úì Retornar original o vac√≠o
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **Robustez:** BUENA
- **Nota:** Sistema inteligente para mercados il√≠quidos

---

### PROCESO 5: Interpolaci√≥n a Punto Fijo

**Ubicaci√≥n:** L√≠neas 380-451
**Complejidad:** ‚≠ê‚≠ê‚≠ê ALTA
**Funci√≥n:** `interpolate_to_fixed_point()`

#### Algoritmo
```python
Target: (delta_rep, dte_rep) del bucket

1. Calcular distancias:
   delta_dist = |delta_actual - delta_target|
   dte_dist = |dte_actual - dte_target|

2. Distancia total normalizada:
   total_dist = sqrt((delta_dist/10)¬≤ + (dte_dist/10)¬≤)

3. Ordenar por total_dist (ascendente)

4. Casos:
   a) n=1 contrato: usar directo
   b) n‚â•2: usar top-3 contratos

5. Interpolaci√≥n weighted (m√©todo='weighted'):
   weights = 1 / (total_dist + 0.01)
   weights_norm = weights / sum(weights)

   IV_interp = Œ£(IV_i √ó weight_i)

6. Calidad seg√∫n distancia m√≠nima:
   < 1.0 ‚Üí EXCELLENT
   < 3.0 ‚Üí GOOD
   < 5.0 ‚Üí FAIR
   ‚â• 5.0 ‚Üí POOR
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **Robustez:** ALTA
- **M√©todo:** Inverse Distance Weighting (IDW) apropiado
- **Nota:** El factor 0.01 previene divisi√≥n por cero correctamente

---

### PROCESO 6: C√°lculo de SKEW Robusto

**Ubicaci√≥n:** L√≠neas 616-657
**Complejidad:** ‚≠ê‚≠ê‚≠ê ALTA
**Funci√≥n:** `calculate_robust_skew()`

#### Algoritmo
```python
Method: 'robust' (regresi√≥n lineal)

1. Calcular log-moneyness:
   PUT:  ln_m = ln(K_ATM / K_strike)
   CALL: ln_m = ln(K_strike / K_ATM)

2. Filtrar |ln_m| > LN_RATIO_EPS (1e-4)

3. Regresi√≥n lineal:
   Y = IV - IV_ATM
   X = ln_moneyness

   Fit: Y = slope √ó X + intercept

4. SKEW_NORM = slope
   (pendiente de la IV smile/smirk)
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **Robustez:** ALTA
- **M√©todo:** Regresi√≥n lineal apropiada para SKEW
- **Nota:** Filtro de ln_m evita divisi√≥n por cero cerca de ATM

---

### PROCESO 7: Forward-Fill Controlado

**Ubicaci√≥n:** L√≠neas 708-789
**Complejidad:** ‚≠ê‚≠ê‚≠ê ALTA
**Funci√≥n:** `reindex_and_ffill_controlled()`

#### Algoritmo
```python
1. Determinar rango efectivo:
   start_eff = max(start_global, bucket_first_date)
   # ‚ö†Ô∏è CR√çTICO: No crea filas antes del primer dato real

2. Generar calendario trading completo [start_eff, end]

3. Reindex DataFrame a calendario completo

4. Marcar datos reales:
   IS_REAL_DATA = IV_bucket.notna()

5. Forward-fill con l√≠mite:
   ffill(limit=MAX_FFILL_DAYS)  # max 30 d√≠as

6. Calcular DAYS_SINCE_REAL_DATA

7. Etiquetar calidad:
   0 d√≠as ‚Üí REAL
   1-10 d√≠as ‚Üí FRESH
   11-30 d√≠as ‚Üí AGED
   >30 d√≠as ‚Üí STALE
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **Robustez:** EXCELENTE
- **Fix V18.1:** Corrige bug de "filas fantasma" correctamente
- **Control de calidad:** EXCELENTE (4 niveles de calidad)

---

### PROCESO 8: C√°lculo de HV (Historical Volatility)

**Ubicaci√≥n:** L√≠neas 1072-1108
**Complejidad:** ‚≠ê‚≠ê‚≠ê MEDIA
**Funci√≥n:** `calculate_hv_vrp()`

#### Algoritmo
```python
1. Calcular retornos logar√≠tmicos:
   ret_log = ln(S_t / S_{t-1})

2. Para cada ventana W ‚àà {7, 21, 63, 252}:

   HV_W = std(ret_log, window=W) √ó sqrt(252)

   Con min_periods = max(3, W/2)

3. Lag de HV:
   HV_7D_VOL(t-1) = shift(HV_7D_VOL, 1)
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **F√≥rmula HV:** EST√ÅNDAR (anualizaci√≥n correcta con ‚àö252)
- **Min_periods:** RAZONABLE (50% de ventana)

---

### PROCESO 9: C√°lculo de VRP (Volatility Risk Premium)

**Ubicaci√≥n:** L√≠neas 1098-1105
**Complejidad:** ‚≠ê‚≠ê MEDIA

#### Algoritmo
```python
1. Forward-fill IV_ATM por bucket:
   IV_ATM_filled = ffill(IV_ATM_bucket, limit=30)

2. Calcular VRP:
   VRP_7D_VOL = IV_ATM_filled - HV_7D_VOL(t-1)
   VRP_7D_VAR = IV_ATM_filled¬≤ - HV_7D_VOL(t-1)¬≤
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **Definici√≥n VRP:** EST√ÅNDAR (Implied - Realized)
- **Lag correcto:** Usa HV(t-1) para evitar lookahead bias

---

### PROCESO 10: C√°lculo de Z-Scores de IV

**Ubicaci√≥n:** L√≠neas 1111-1173
**Complejidad:** ‚≠ê‚≠ê‚≠ê MEDIA
**Funci√≥n:** `calculate_iv_zscores()`

#### Algoritmo
```python
Para cada ventana W ‚àà {20, 63, 252}:

1. Calcular SMA:
   IV_SMA_W = rolling_mean(IV_ATM, window=W)
   Con min_periods = max(15, W/3)

2. Calcular SD:
   IV_SD_W = rolling_std(IV_ATM, window=W)

3. Calcular Z-score:
   IV_Z_W = (IV_ATM - IV_SMA_W) / IV_SD_W

   Con protecci√≥n: if IV_SD_W = 0 ‚Üí NaN
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **F√≥rmula Z-score:** EST√ÅNDAR
- **Manejo divisi√≥n por cero:** CORRECTO
- **Min_periods:** CONSERVADOR (W/3, m√≠nimo 15)

---

### PROCESO 11: üî¥ **CR√çTICO** - C√°lculo de Percentiles Hist√≥ricos

**Ubicaci√≥n:** L√≠neas 510-570
**Complejidad:** ‚≠ê‚≠ê‚≠ê‚≠ê MUY ALTA
**Funci√≥n:** `rolling_percentile_with_universal_calendar()`

#### Algoritmo Actual
```python
Para cada fecha t:

1. Obtener valor actual: value_t
2. Obtener calendario antes de t:
   calendar_window = last_W_trading_days_before(t)

3. Filtrar datos hist√≥ricos:
   historical = data[
       date in calendar_window AND
       IS_REAL_DATA = True AND
       value.notna()
   ]

4. Calcular percentil emp√≠rico:
   percentile = (historical < value_t).sum() / len(historical)

5. Validaci√≥n:
   - Si len(historical) < min_required ‚Üí NaN
   - min_required = max(int(W √ó 0.70), 5)
```

#### üî¥ ERROR CR√çTICO #1: C√°lculo de Percentil Incorrecto

**L√≠nea 567:**
```python
percentile = (historical < current_value).sum() / len(historical)
```

**Problema:**
1. **Definici√≥n incorrecta de percentil emp√≠rico**
2. Usa `<` (strictly less than) en lugar de `<=`
3. No incluye el valor actual en el denominador

**F√≥rmula actual:**
```
P = #{x ‚àà historical : x < value_t} / N
```

**F√≥rmula correcta (percentil emp√≠rico est√°ndar):**
```
P = (#{x ‚àà historical : x < value_t} + 0.5 √ó #{x = value_t}) / N
```

O alternativamente (m√©todo rank):
```
P = rank(value_t) / (N + 1)
```

**Impacto:**

| Caso | Valor Actual | Hist√≥ricos | Percentil Actual | Percentil Correcto | Error |
|------|--------------|------------|------------------|-------------------|-------|
| M√≠nimo | 15.0 | [15.0, 20.0, 25.0, 30.0] | 0/4 = 0% | ~12.5% | -12.5% |
| M√°ximo | 30.0 | [15.0, 20.0, 25.0, 30.0] | 3/4 = 75% | ~87.5% | -12.5% |
| Mediana | 22.5 | [15.0, 20.0, 25.0, 30.0] | 2/4 = 50% | ~50% | 0% |

**Consecuencias:**
- ‚ö†Ô∏è Percentiles en los extremos (0-10%, 90-100%) **subestimados sistem√°ticamente**
- ‚ö†Ô∏è Contratos muy baratos (percentil 0-5%) aparecer√°n artificialmente m√°s caros
- ‚ö†Ô∏è Contratos muy caros (percentil 95-100%) aparecer√°n artificialmente m√°s baratos
- ‚ö†Ô∏è **Clasificaci√≥n ULTRA_BARATA y ULTRA_CARA sesgada**

**Severidad:** üî¥ **CR√çTICA**
**Urgencia:** ALTA
**Afectaci√≥n:** Todos los scores y clasificaciones

**Correcci√≥n Recomendada:**
```python
# Opci√≥n 1: Percentil emp√≠rico con empates
n_below = (historical < current_value).sum()
n_equal = (historical == current_value).sum()
percentile = (n_below + 0.5 * n_equal) / len(historical)

# Opci√≥n 2: Usar scipy (m√°s robusto)
from scipy.stats import percentileofscore
percentile = percentileofscore(historical, current_value, kind='mean') / 100.0
```

---

### PROCESO 12: C√°lculo de Cobertura Temporal

**Ubicaci√≥n:** L√≠neas 573-611
**Complejidad:** ‚≠ê‚≠ê MEDIA
**Funci√≥n:** `calculate_coverage_metrics()`

#### Algoritmo
```python
Para cada fecha t:

1. Obtener ventana de W d√≠as trading antes de t

2. Contar d√≠as con datos reales:
   n_with_data = count(
       date in window AND IS_REAL_DATA = True
   )

3. Calcular cobertura:
   coverage = n_with_data / W
```

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **M√©trica:** Clara y √∫til para validaci√≥n

---

### PROCESO 13: üî¥ **CR√çTICO** - C√°lculo de Scores Combinados

**Ubicaci√≥n:** L√≠neas 956-1069, espec√≠ficamente 1035-1047
**Complejidad:** ‚≠ê‚≠ê‚≠ê‚≠ê MUY ALTA
**Funci√≥n:** `calculate_bucket_percentiles()`

#### Algoritmo
```python
Pesos globales: w_iv=0.60, w_sk=0.35, w_vrp=0.05

Para cada bucket (wing, delta_code, dte_code):

1. Calcular percentiles:
   - IV_pct_W
   - SKEW_pct_W
   - VRP_pct_W

2. Determinar si es ATM:
   is_atmish = (40 <= delta_rep <= 60)

3. Calcular SCORE:

   if is_atmish:
       # ‚ö†Ô∏è Renormalizaci√≥n de pesos
       denom = w_iv + w_vrp  # = 0.65
       wiv = w_iv / denom    # = 0.60/0.65 = 0.923
       wvr = w_vrp / denom   # = 0.05/0.65 = 0.077

       SCORE = wiv √ó IV_pct + wvr √ó VRP_pct

   else:  # OTM
       SCORE = w_iv √ó IV_pct + w_sk √ó SKEW_pct + w_vrp √ó VRP_pct
```

#### üî¥ ERROR CR√çTICO #2: Incomparabilidad de Scores ATM vs OTM

**Problema:**

1. **Scores ATM usan pesos renormalizados:**
   - IV: 92.3% (vs 60% nominal)
   - VRP: 7.7% (vs 5% nominal)
   - SKEW: 0% (excluido)

2. **Scores OTM usan pesos nominales:**
   - IV: 60%
   - SKEW: 35%
   - VRP: 5%

3. **Consecuencia:** Un score de 0.50 ATM **NO significa lo mismo** que 0.50 OTM

**Ejemplo Num√©rico:**

Supongamos:
- IV_pct = 0.50 (mediana)
- SKEW_pct = 1.00 (muy alto)
- VRP_pct = 0.50 (mediana)

**Score OTM (delta=10):**
```
SCORE_OTM = 0.60√ó0.50 + 0.35√ó1.00 + 0.05√ó0.50
          = 0.30 + 0.35 + 0.025
          = 0.675 ‚Üí LABEL = "CARA"
```

**Score ATM (delta=50):**
```
SCORE_ATM = 0.923√ó0.50 + 0.077√ó0.50
          = 0.4615 + 0.0385
          = 0.50 ‚Üí LABEL = "LIGERAMENTE_BARATA"
```

**Problema:**
- Mismo IV_pct y VRP_pct
- OTM tiene SKEW muy alto (se√±al de caro)
- Pero se clasifican diferente: ATM="BARATA", OTM="CARA"
- **Los scores no son comparables cross-bucket**

**Impacto:**
- ‚ö†Ô∏è Imposible comparar directamente "qu√© tan caro" est√° un contrato ATM vs OTM
- ‚ö†Ô∏è Estrategias que comparan scores entre diferentes deltas son incorrectas
- ‚ö†Ô∏è Rankings agregados mezclando ATM y OTM est√°n sesgados

**Severidad:** üî¥ **CR√çTICA**
**Urgencia:** ALTA
**Afectaci√≥n:** Comparaciones cross-bucket, rankings globales

**Posibles Soluciones:**

**Opci√≥n A: Usar pesos consistentes (recomendado)**
```python
# Usar siempre los mismos pesos, incluso si SKEW_pct es NaN para ATM
SCORE = w_iv √ó IV_pct + w_sk √ó SKEW_pct + w_vrp √ó VRP_pct

# Para ATM, SKEW_pct ser√° NaN ‚Üí contribuir√° 0 autom√°ticamente
# Pero la escala se mantiene consistente
```

**Opci√≥n B: Crear scores separados**
```python
# Tener SCORE_ATM y SCORE_OTM como m√©tricas diferentes
# No compararlos directamente
SCORE_ATM = 0.923 √ó IV_pct + 0.077 √ó VRP_pct
SCORE_OTM = 0.60 √ó IV_pct + 0.35 √ó SKEW_pct + 0.05 √ó VRP_pct
```

**Opci√≥n C: Normalizar post-c√°lculo**
```python
# Escalar SCORE_ATM para que rango [0,1] coincida con distribuci√≥n OTM
# Requiere an√°lisis emp√≠rico
```

---

### PROCESO 14: üü° C√°lculo de TERM_bucket

**Ubicaci√≥n:** L√≠neas 1612-1614
**Complejidad:** ‚≠ê BAJA

#### C√≥digo Actual
```python
"TERM_bucket": (
    iv_atm - (np.nan if np.isnan(IV_ATM_30D) else IV_ATM_30D)
)
```

#### üü° ERROR MENOR: Expresi√≥n Redundante

**Problema:**
```python
(np.nan if np.isnan(IV_ATM_30D) else IV_ATM_30D)
```

Esto es equivalente a simplemente `IV_ATM_30D`:
- Si `IV_ATM_30D` es NaN ‚Üí expresi√≥n devuelve NaN
- Si `IV_ATM_30D` es num√©rico ‚Üí expresi√≥n devuelve IV_ATM_30D

**Simplificaci√≥n:**
```python
"TERM_bucket": iv_atm - IV_ATM_30D
```

**Nota:** NumPy/Pandas ya propagan NaN correctamente en restas.

**Impacto:** NINGUNO (solo legibilidad)
**Severidad:** üü° BAJA
**Urgencia:** BAJA

---

### PROCESO 15: Clasificaci√≥n en Niveles y Labels

**Ubicaci√≥n:** L√≠neas 336-352
**Complejidad:** ‚≠ê BAJA
**Funciones:** `level10_from_score()`, `label10_from_score()`

#### Algoritmo
```python
Score ‚Üí Level:

1. Clip score a [0, 1]:
   s_clipped = max(0, min(1, score))

2. Calcular nivel:
   level = floor(10 √ó s_clipped) + 1
   level = min(level, 10)

3. Mapear a label:
   LABEL10_NAMES[level - 1]
```

#### Mapeo Score ‚Üí Level ‚Üí Label

| Score Range | Level | Label |
|-------------|-------|-------|
| [0.00, 0.10) | 1 | ULTRA_BARATA |
| [0.10, 0.20) | 2 | MUY_BARATA |
| [0.20, 0.30) | 3 | BARATA |
| [0.30, 0.40) | 4 | ALGO_BARATA |
| [0.40, 0.50) | 5 | LIGERAMENTE_BARATA |
| [0.50, 0.60) | 6 | LIGERAMENTE_CARA |
| [0.60, 0.70) | 7 | ALGO_CARA |
| [0.70, 0.80) | 8 | CARA |
| [0.80, 0.90) | 9 | MUY_CARA |
| [0.90, 1.00] | 10 | ULTRA_CARA |

#### ‚úÖ DIAGN√ìSTICO
- **Estado:** CORRECTO
- **L√≥gica:** Clara y sim√©trica
- **Nota:** Score 1.00 ‚Üí level 10 (caso borde manejado correctamente)

---

## üêõ RESUMEN DE ERRORES DETECTADOS

### üî¥ ERRORES CR√çTICOS (3)

#### 1. üî¥ **PERCENTIL EMP√çRICO INCORRECTO** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
- **Ubicaci√≥n:** L√≠nea 567
- **Funci√≥n:** `rolling_percentile_with_universal_calendar()`
- **Problema:** Usa `(historical < value).sum() / N` en lugar de percentil emp√≠rico correcto
- **Impacto:**
  - Percentiles extremos (0-10%, 90-100%) subestimados ~12.5%
  - Clasificaciones ULTRA_BARATA y ULTRA_CARA sesgadas
  - Todos los scores afectados
- **Severidad:** üî¥ CR√çTICA
- **Probabilidad fix rompa c√≥digo:** BAJA (cambio local)

#### 2. üî¥ **SCORES ATM vs OTM NO COMPARABLES** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
- **Ubicaci√≥n:** L√≠neas 1035-1047
- **Funci√≥n:** `calculate_bucket_percentiles()`
- **Problema:** Pesos renormalizados para ATM (92.3% IV vs 60% nominal)
- **Impacto:**
  - Imposible comparar scores cross-bucket (ATM vs OTM)
  - Rankings agregados sesgados
  - Estrategias multi-strike incorrectas
- **Severidad:** üî¥ CR√çTICA
- **Probabilidad fix rompa c√≥digo:** MEDIA (requiere decisi√≥n de dise√±o)

#### 3. üü° **EXPRESI√ìN REDUNDANTE EN TERM_bucket**
- **Ubicaci√≥n:** L√≠nea 1613
- **Problema:** `(np.nan if np.isnan(x) else x)` es redundante
- **Impacto:** NINGUNO (solo legibilidad)
- **Severidad:** üü° BAJA
- **Correcci√≥n:** Trivial

### üü° ADVERTENCIAS MENORES (8)

#### 4. üü° Inconsistencia en bordes de buckets
- **Ubicaci√≥n:** L√≠neas 467-475
- **Problema:** √öltimo bucket usa `<=`, otros usan `<`
- **Impacto:** BAJO (solo valores exactos en l√≠mites)
- **Severidad:** üü° BAJA

#### 5. üü° Interpolaci√≥n sin validaci√≥n de convexidad
- **Ubicaci√≥n:** Funci√≥n `interpolate_to_fixed_point()`
- **Problema:** No valida que IV interpolada respete no-arbitraje
- **Impacto:** BAJO (casos raros)
- **Severidad:** üü° BAJA

#### 6. üü° Z-scores con ventanas fijas (no adaptativas)
- **Ubicaci√≥n:** L√≠neas 1125-1155
- **Problema:** min_periods fijo puede dar Z-scores prematuros
- **Impacto:** BAJO (primeros d√≠as de cada bucket)
- **Severidad:** üü° BAJA

#### 7. üü° HV anualizaci√≥n asume 252 d√≠as (fijo)
- **Ubicaci√≥n:** L√≠nea 1087
- **Problema:** Factor `sqrt(252)` asume 252 d√≠as trading/a√±o (puede variar 251-253)
- **Impacto:** M√çNIMO (~0.4% error m√°ximo)
- **Severidad:** üü° MUY BAJA

#### 8. üü° VRP usa solo ventana 7D
- **Ubicaci√≥n:** L√≠nea 1103
- **Problema:** `VRP_7D` es la √∫nica VRP calculada (podr√≠a tener 21D, 63D)
- **Impacto:** NINGUNO (elecci√≥n de dise√±o razonable)
- **Severidad:** ‚úÖ NO ES ERROR (sugerencia)

#### 9. üü° Coverage m√≠nima 70% puede ser estricta
- **Ubicaci√≥n:** L√≠nea 536, config `MIN_PERCENTILE_COVERAGE = 0.70`
- **Problema:** En mercados il√≠quidos, puede descartar muchos buckets
- **Impacto:** MEDIO en mercados il√≠quidos
- **Severidad:** üü° BAJA (configurable)

#### 10. üü° Forward-fill 30 d√≠as puede ser excesivo
- **Ubicaci√≥n:** Config `MAX_FFILL_DAYS = 30`
- **Problema:** Datos de 30 d√≠as pueden estar muy stale
- **Impacto:** MEDIO (marcado como AGED/STALE, pero a√∫n usado)
- **Severidad:** üü° BAJA (configurable, con etiquetas)

#### 11. üü° No hay validaci√≥n de calendario USA post-2025
- **Ubicaci√≥n:** L√≠neas 198-213
- **Problema:** Calendario holidays solo hasta 2025
- **Impacto:** CR√çTICO despu√©s de 2025
- **Severidad:** üü° MEDIA (requiere actualizaci√≥n anual)

---

## ‚úÖ FORTALEZAS DEL SISTEMA

### Arquitectura
1. ‚úÖ **Dise√±o modular** excelente (funciones bien separadas)
2. ‚úÖ **Logging exhaustivo** (niveles INFO/WARNING/ERROR apropiados)
3. ‚úÖ **Configuraci√≥n centralizada** (par√°metros en constantes globales)
4. ‚úÖ **Manejo robusto de errores** (try/except, validaciones)

### Procesamiento de Datos
5. ‚úÖ **Validaci√≥n de esquema** completa (l√≠nea 355-361)
6. ‚úÖ **Normalizaci√≥n robusta** de formatos (ms_of_day, delta escala)
7. ‚úÖ **Forward-fill controlado** con l√≠mites y etiquetas de calidad
8. ‚úÖ **Eliminaci√≥n de filas fantasma** (Fix V18.1)
9. ‚úÖ **Reindex desde primer dato real** (evita padding inicial in√∫til)

### C√°lculos Matem√°ticos
10. ‚úÖ **Calendario universal USA** para percentiles comparables
11. ‚úÖ **Interpolaci√≥n IDW** apropiada para puntos fijos
12. ‚úÖ **SKEW robusto** con regresi√≥n lineal (mejor que ratios simples)
13. ‚úÖ **HV anualizaci√≥n** correcta (√ó‚àö252)
14. ‚úÖ **VRP con lag** correcto (evita lookahead bias)
15. ‚úÖ **Z-scores** con protecci√≥n divisi√≥n por cero

### Filtros de Calidad
16. ‚úÖ **Filtros de spread** (absoluto y porcentual)
17. ‚úÖ **Filtro ask/bid ratio** (max 10x)
18. ‚úÖ **M√≠nimo contratos por bucket** (N_MIN=3)
19. ‚úÖ **Expansi√≥n inteligente** a vecinos si datos insuficientes
20. ‚úÖ **Validaciones de monotonicity** y arbitraje (aunque opcionales)

### M√©tricas y Reportes
21. ‚úÖ **Coverage metrics** por ventana
22. ‚úÖ **Quality report** exhaustivo (validate_surface_quality)
23. ‚úÖ **M√©tricas de interpolaci√≥n** (quality, n_contracts_used)
24. ‚úÖ **Tracking de expansion_level**

### Modo Incremental
25. ‚úÖ **Rec√°lculo de cola optimizado** (solo √∫ltimos N d√≠as)
26. ‚úÖ **Detecci√≥n de archivos nuevos**
27. ‚úÖ **Merge con datos existentes**

### V19 PERMA Features
28. ‚úÖ **Lockfile para instancia √∫nica** (evita ejecuciones simult√°neas)
29. ‚úÖ **Auto-loop con scheduler** configurable
30. ‚úÖ **Detecci√≥n de lock stale** (>12h o PID muerto)

---

## üìã CLASIFICACI√ìN DE SEVERIDAD

### Matriz de Severidad

| ID | Error/Advertencia | Impacto | Frecuencia | Severidad Final | Urgencia |
|----|-------------------|---------|------------|-----------------|----------|
| 1 | Percentil incorrecto | ALTO | 100% | üî¥ CR√çTICA | ALTA |
| 2 | Scores ATM vs OTM | ALTO | ~30% buckets | üî¥ CR√çTICA | ALTA |
| 3 | TERM redundancia | NINGUNO | 100% | üü° BAJA | BAJA |
| 4 | Bordes buckets | BAJO | <1% | üü° BAJA | BAJA |
| 5 | Interpolaci√≥n convexidad | BAJO | <5% | üü° BAJA | MEDIA |
| 6 | Z-scores prematuros | BAJO | Primeros d√≠as | üü° BAJA | BAJA |
| 7 | HV anualizaci√≥n fija | M√çNIMO | 100% | üü° MUY BAJA | BAJA |
| 8 | VRP solo 7D | N/A | N/A | ‚úÖ DISE√ëO | N/A |
| 9 | Coverage 70% estricta | MEDIO | Il√≠quidos | üü° BAJA | BAJA |
| 10 | FFILL 30d excesivo | MEDIO | Gaps grandes | üü° BAJA | BAJA |
| 11 | Calendario post-2025 | CR√çTICO | Post-2025 | üü° MEDIA | MEDIA |

### Priorizaci√≥n de Fixes

**üî¥ PRIORIDAD M√ÅXIMA (Cr√≠tico - Urgente):**
1. **Fix #1:** Corregir c√°lculo de percentil emp√≠rico (L√≠nea 567)
2. **Fix #2:** Unificar scores ATM/OTM o separarlos expl√≠citamente (L√≠neas 1035-1047)

**üü° PRIORIDAD MEDIA (Prevenci√≥n):**
3. **Fix #11:** Extender calendario USA hasta 2030+ (L√≠neas 198-213)
4. **Fix #5:** A√±adir validaci√≥n de convexidad post-interpolaci√≥n

**üü¢ PRIORIDAD BAJA (Mejora):**
5. **Fix #3:** Simplificar TERM_bucket (cosm√©tico)
6. **Fix #4:** Unificar l√≥gica de bordes de buckets
7. Revisar min_periods de Z-scores (hacer adaptativo)

---

## üîß RECOMENDACIONES ESPEC√çFICAS

### 1. Correcci√≥n de Percentil (URGENTE)

**Archivo:** L√≠nea 567

**Cambio:**
```python
# ANTES (INCORRECTO)
percentile = (historical < current_value).sum() / len(historical)

# DESPU√âS (CORRECTO - Opci√≥n scipy recomendada)
from scipy.stats import percentileofscore
percentile = percentileofscore(historical.values, current_value, kind='mean') / 100.0

# O alternativamente (sin scipy):
n_below = (historical < current_value).sum()
n_equal = (historical == current_value).sum()
percentile = (n_below + 0.5 * n_equal) / len(historical)
```

**Validaci√≥n post-fix:**
```python
# Test cases
assert percentile_correcto([10, 20, 30], 10) ‚âà 0.125  # Min
assert percentile_correcto([10, 20, 30], 20) ‚âà 0.50   # Median
assert percentile_correcto([10, 20, 30], 30) ‚âà 0.875  # Max
```

### 2. Unificaci√≥n de Scores (URGENTE)

**Opci√≥n A: Pesos consistentes (recomendada)**

**Archivo:** L√≠neas 1035-1047

```python
# ANTES (INCONSISTENTE)
if is_atmish:
    denom = (w_iv + w_vrp)
    wiv = w_iv / denom
    wvr = w_vrp / denom
    SCORE = wiv * IV_pct + wvr * VRP_pct
else:
    SCORE = w_iv * IV_pct + w_sk * SKEW_pct + w_vrp * VRP_pct

# DESPU√âS (CONSISTENTE)
# Rellenar SKEW_pct con 0.5 (neutral) para ATM si es NaN
SKEW_pct_filled = gg[f"SKEW_pct_{W}"].fillna(0.5)

# Usar siempre la misma f√≥rmula
SCORE = w_iv * IV_pct + w_sk * SKEW_pct_filled + w_vrp * VRP_pct
```

**Opci√≥n B: Scores separados (alternativa)**

```python
# Crear dos m√©tricas diferentes con nombres distintos
if is_atmish:
    SCORE_ATM = (w_iv / (w_iv + w_vrp)) * IV_pct + (w_vrp / (w_iv + w_vrp)) * VRP_pct
else:
    SCORE_OTM = w_iv * IV_pct + w_sk * SKEW_pct + w_vrp * VRP_pct

# NO compararlos directamente
# Labels separados: LABEL_ATM, LABEL_OTM
```

### 3. Extensi√≥n de Calendario USA

**Archivo:** L√≠neas 198-213

```python
# A√±adir holidays 2026-2030
USA_HOLIDAYS_2026_2030 = [
    # 2026
    "2026-01-01", "2026-01-19", "2026-02-16", "2026-04-03", "2026-05-25",
    "2026-06-19", "2026-07-03", "2026-09-07", "2026-11-26", "2026-12-25",
    # 2027-2030 (consultar NYSE calendar)
    # ...
]

USA_HOLIDAYS.extend(USA_HOLIDAYS_2026_2030)
USA_HOLIDAYS_SET = set(pd.to_datetime(USA_HOLIDAYS).date)
```

### 4. Simplificaci√≥n TERM_bucket

**Archivo:** L√≠nea 1613

```python
# ANTES
"TERM_bucket": (
    iv_atm - (np.nan if np.isnan(IV_ATM_30D) else IV_ATM_30D)
)

# DESPU√âS
"TERM_bucket": iv_atm - IV_ATM_30D
```

### 5. Unificaci√≥n de Bordes de Buckets

**Archivo:** L√≠neas 467-475

```python
# ANTES (inconsistente)
if db is DELTA_BUCKETS[-1]:
    sub = bloc_w.loc[
        (bloc_w["delta_abs"] >= low_d) & (bloc_w["delta_abs"] <= high_d) & ...
    ]
else:
    sub = bloc_w.loc[
        (bloc_w["delta_abs"] >= low_d) & (bloc_w["delta_abs"] < high_d) & ...
    ]

# DESPU√âS (consistente)
# Ajustar high del √∫ltimo bucket a un valor inalcanzable (e.g., 100.1)
DELTA_BUCKETS[-1]["high"] = 100.1  # En config

# Usar siempre <
sub = bloc_w.loc[
    (bloc_w["delta_abs"] >= low_d) & (bloc_w["delta_abs"] < high_d) & ...
]
```

---

## üìä AN√ÅLISIS DE IMPACTO

### Impacto de Fix #1 (Percentil)

**Escenarios afectados:**

| Percentil Hist√≥rico | Score Actual | Score Corregido | Œî Label |
|---------------------|--------------|-----------------|---------|
| 0-10% (ULTRA_BARATA) | 0.00-0.10 | 0.10-0.20 | +1 nivel |
| 10-20% (MUY_BARATA) | 0.10-0.20 | 0.15-0.25 | 0-1 nivel |
| 40-60% (MEDIANA) | 0.40-0.60 | 0.40-0.60 | Sin cambio |
| 80-90% (MUY_CARA) | 0.80-0.90 | 0.75-0.88 | 0-1 nivel |
| 90-100% (ULTRA_CARA) | 0.90-1.00 | 0.80-0.95 | -1 nivel |

**Estimaci√≥n:**
- ~15-20% de clasificaciones cambiar√°n en 1 nivel
- ~3-5% cambiar√°n en 2 niveles
- Cambios concentrados en extremos (percentiles <10% y >90%)

### Impacto de Fix #2 (Scores ATM/OTM)

**Con Opci√≥n A (pesos consistentes):**
- Scores ATM bajar√°n ~10-15% en promedio
- M√°s contratos ATM clasificados como "baratos"
- Rankings cross-bucket se volver√°n comparables

**Con Opci√≥n B (scores separados):**
- Sin cambio en valores num√©ricos
- Requiere cambios en c√≥digo downstream (dashboards, estrategias)
- Mayor claridad conceptual

---

## üéØ PLAN DE VALIDACI√ìN POST-FIX

### Tests Unitarios Recomendados

```python
def test_percentile_correcto():
    """Validar fix de percentil emp√≠rico"""

    # Test 1: Valor m√≠nimo
    hist = np.array([10, 20, 30, 40])
    assert abs(percentile_new(hist, 10) - 0.125) < 0.01

    # Test 2: Valor m√°ximo
    assert abs(percentile_new(hist, 40) - 0.875) < 0.01

    # Test 3: Valor mediano
    assert abs(percentile_new(hist, 25) - 0.50) < 0.05

    # Test 4: Empates
    hist_dup = np.array([10, 20, 20, 30])
    p = percentile_new(hist_dup, 20)
    assert 0.375 < p < 0.625  # Debe estar en rango central

def test_scores_consistentes():
    """Validar que scores ATM y OTM usan misma escala"""

    # Mock data
    iv_pct = 0.50
    skew_pct = 0.50
    vrp_pct = 0.50

    # Calcular ambos
    score_atm = calc_score(iv_pct, None, vrp_pct, is_atmish=True)
    score_otm = calc_score(iv_pct, skew_pct, vrp_pct, is_atmish=False)

    # Ambos deben dar mismo score si SKEW=0.5 (neutral)
    assert abs(score_atm - score_otm) < 0.05

def test_calendario_completo():
    """Validar que calendario cubre rango de datos"""

    max_date = df['date'].max()
    assert max_date.year <= 2025 or len(USA_HOLIDAYS) > 100
```

### Validaci√≥n Emp√≠rica

```python
# 1. Comparar distribuciones antes/despu√©s
df_old = pd.read_parquet("surface_metrics_OLD.parquet")
df_new = pd.read_parquet("surface_metrics_NEW.parquet")

# Distribuci√≥n de percentiles
for W in [7, 21, 63, 252]:
    col = f'IV_pct_{W}'

    plt.figure(figsize=(12, 4))

    plt.subplot(1, 3, 1)
    plt.hist(df_old[col].dropna(), bins=50, alpha=0.5, label='OLD')
    plt.hist(df_new[col].dropna(), bins=50, alpha=0.5, label='NEW')
    plt.title(f'{col} Distribution')
    plt.legend()

    plt.subplot(1, 3, 2)
    plt.scatter(df_old[col], df_new[col], alpha=0.1)
    plt.plot([0, 1], [0, 1], 'r--')
    plt.title('OLD vs NEW')

    plt.subplot(1, 3, 3)
    diff = df_new[col] - df_old[col]
    plt.hist(diff.dropna(), bins=50)
    plt.title('Difference (NEW - OLD)')
    plt.axvline(0, color='r', linestyle='--')

    plt.tight_layout()
    plt.savefig(f'validation_{col}.png')

# 2. An√°lisis de cambios en labels
merge = df_old.merge(
    df_new[['date', 'wing', 'delta_code', 'dte_code', 'LABEL10_SIMPLE_63']],
    on=['date', 'wing', 'delta_code', 'dte_code'],
    suffixes=('_old', '_new')
)

changed = merge[merge['LABEL10_SIMPLE_63_old'] != merge['LABEL10_SIMPLE_63_new']]
print(f"Labels changed: {len(changed)} / {len(merge)} ({len(changed)/len(merge)*100:.1f}%)")

# Matriz de transici√≥n
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(
    merge['LABEL10_SIMPLE_63_old'].map(lambda x: LABEL10_NAMES.index(x) if x in LABEL10_NAMES else -1),
    merge['LABEL10_SIMPLE_63_new'].map(lambda x: LABEL10_NAMES.index(x) if x in LABEL10_NAMES else -1)
)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=LABEL10_NAMES, yticklabels=LABEL10_NAMES)
plt.title('Label Transition Matrix (OLD ‚Üí NEW)')
plt.ylabel('OLD')
plt.xlabel('NEW')
plt.savefig('label_transitions.png')
```

---

## üìà M√âTRICAS DE ROBUSTEZ

### An√°lisis de Cobertura

```python
# Generar reporte de cobertura por bucket
coverage_report = df.groupby(['wing', 'delta_code', 'dte_code']).agg({
    'IS_REAL_DATA': 'sum',
    'date': 'count',
    'coverage_63D': 'mean',
    'IV_pct_63': lambda x: x.notna().sum()
}).reset_index()

coverage_report['real_pct'] = coverage_report['IS_REAL_DATA'] / coverage_report['date'] * 100

# Buckets problem√°ticos (coverage < 50%)
problematic = coverage_report[coverage_report['coverage_63D'] < 0.5]
print(f"Buckets con coverage < 50%: {len(problematic)}")
print(problematic[['wing', 'delta_code', 'dte_code', 'coverage_63D', 'real_pct']])
```

### An√°lisis de Calidad de Interpolaci√≥n

```python
# Distribuci√≥n de calidad de interpolaci√≥n
quality_dist = df['interpolation_quality'].value_counts()
print("Interpolation Quality Distribution:")
print(quality_dist)
print(f"\nEXCELLENT: {quality_dist.get('EXCELLENT', 0) / len(df) * 100:.1f}%")
print(f"GOOD: {quality_dist.get('GOOD', 0) / len(df) * 100:.1f}%")
print(f"FAIR: {quality_dist.get('FAIR', 0) / len(df) * 100:.1f}%")
print(f"POOR: {quality_dist.get('POOR', 0) / len(df) * 100:.1f}%")

# Contratos promedio usados
print(f"\nContratos promedio por bucket: {df['n_contracts_used'].mean():.1f}")
print(f"Mediana: {df['n_contracts_used'].median():.0f}")
```

---

## üèÜ CONCLUSIONES FINALES

### Resumen

**V19 PERMA SURFACE es un sistema robusto y bien dise√±ado** con:

‚úÖ **FORTALEZAS:**
- Arquitectura modular y mantenible
- Manejo exhaustivo de casos edge
- Validaciones de calidad multi-nivel
- Forward-fill controlado inteligente
- Calendario universal para comparabilidad
- M√©tricas de cobertura y calidad

üî¥ **DEBILIDADES CR√çTICAS:**
- **C√°lculo de percentil emp√≠rico incorrecto** (sesgo en extremos)
- **Incomparabilidad de scores ATM vs OTM** (pesos diferentes)

üü° **MEJORAS RECOMENDADAS:**
- Extender calendario USA post-2025
- Validar convexidad post-interpolaci√≥n
- Unificar l√≥gica de bordes de buckets

### Riesgo Actual

| Aspecto | Riesgo | Justificaci√≥n |
|---------|--------|---------------|
| **Precisi√≥n de percentiles** | üî¥ ALTO | Error sistem√°tico en extremos (¬±12.5%) |
| **Comparabilidad scores** | üî¥ ALTO | ATM y OTM usan escalas diferentes |
| **Integridad de datos** | üü¢ BAJO | Validaciones exhaustivas funcionan bien |
| **Robustez operativa** | üü¢ BAJO | Sistema PERMA con lockfile es s√≥lido |
| **Mantenibilidad** | üü¢ BAJO | C√≥digo bien estructurado y documentado |

### Prioridad de Acci√≥n

**URGENTE (< 1 semana):**
1. ‚úÖ Fix c√°lculo de percentil (L√≠nea 567)
2. ‚úÖ Decidir estrategia scores ATM/OTM e implementar

**IMPORTANTE (< 1 mes):**
3. ‚úÖ Extender calendario USA hasta 2030
4. ‚úÖ Implementar tests de validaci√≥n
5. ‚úÖ Ejecutar validaci√≥n emp√≠rica (OLD vs NEW)

**DESEABLE (< 3 meses):**
6. ‚úÖ A√±adir validaci√≥n de convexidad
7. ‚úÖ Revisar y optimizar min_periods
8. ‚úÖ Documentar decisiones de dise√±o

### Nivel de Confianza Post-Fix

**Antes de fixes:**
- Percentiles: üü° ~85% confianza (error en extremos)
- Scores: üü° ~80% confianza (incomparabilidad ATM/OTM)
- Sistema general: üü¢ ~90% confianza

**Despu√©s de fixes cr√≠ticos:**
- Percentiles: üü¢ ~98% confianza
- Scores: üü¢ ~95% confianza
- Sistema general: üü¢ ~97% confianza

---

## üìö AP√âNDICES

### A. Glosario de T√©rminos

| T√©rmino | Definici√≥n |
|---------|------------|
| **Bucket** | Celda de la surface (Delta √ó DTE) |
| **IV percentile** | Posici√≥n del IV actual en distribuci√≥n hist√≥rica |
| **SKEW_NORM** | Pendiente de la IV smile (regresi√≥n ln-moneyness vs IV) |
| **VRP** | Volatility Risk Premium = IV - HV |
| **TERM** | Term structure = IV_bucket - IV_ATM_30D |
| **Coverage** | % de d√≠as con datos reales en ventana rolling |
| **Forward-fill** | Propagaci√≥n de √∫ltimo valor v√°lido |
| **Universal calendar** | Calendario trading USA unificado para comparabilidad |

### B. Referencias Matem√°ticas

**Percentil Emp√≠rico:**
- Hyndman, R. J., & Fan, Y. (1996). Sample Quantiles in Statistical Packages. *The American Statistician*, 50(4), 361-365.

**Volatility Skew:**
- Bergomi, L. (2016). *Stochastic Volatility Modeling*. CRC Press.

**VRP:**
- Bollerslev, T., Tauchen, G., & Zhou, H. (2009). Expected Stock Returns and Variance Risk Premia. *Review of Financial Studies*, 22(11), 4463-4492.

### C. Configuraci√≥n Recomendada Post-Fix

```python
# PAR√ÅMETROS CR√çTICOS
SCORE_WEIGHTS = (0.60, 0.35, 0.05)  # Mantener
WINDOWS = [7, 21, 63, 252]  # Mantener

# PAR√ÅMETROS A REVISAR
MIN_PERCENTILE_COVERAGE = 0.60  # Bajar de 0.70 para mercados il√≠quidos
MAX_FFILL_DAYS = 20  # Bajar de 30 para evitar datos muy stale
N_MIN_PER_BUCKET = 5  # Subir de 3 para mayor robustez

# NUEVOS PAR√ÅMETROS SUGERIDOS
PERCENTILE_METHOD = 'scipy'  # 'scipy' o 'empirical'
SCORE_CONSISTENCY_MODE = 'unified'  # 'unified' o 'separated'
VALIDATE_CONVEXITY = True  # A√±adir validaci√≥n post-interpolaci√≥n
```

---

## üìû CONTACTO Y SEGUIMIENTO

**Analista:** Claude Code
**Fecha An√°lisis:** 2025-11-28
**Versi√≥n Documento:** 1.0

**Pr√≥ximos Pasos:**
1. ‚úÖ Revisar este diagn√≥stico con el equipo
2. ‚úÖ Aprobar estrategia de fixes (percentil + scores)
3. ‚úÖ Implementar fixes en rama de desarrollo
4. ‚úÖ Ejecutar suite de validaci√≥n
5. ‚úÖ Comparar resultados OLD vs NEW
6. ‚úÖ Deploy a producci√≥n con monitoring

**Revisi√≥n Recomendada:** Trimestral (actualizaci√≥n calendario, nuevas mejoras)

---

*Fin del Diagn√≥stico Completo*
